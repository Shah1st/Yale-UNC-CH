{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":39763,"databundleVersionId":11756775,"sourceType":"competition"},{"sourceId":11568812,"sourceType":"datasetVersion","datasetId":7253205},{"sourceId":11569667,"sourceType":"datasetVersion","datasetId":7253605},{"sourceId":11569755,"sourceType":"datasetVersion","datasetId":7253661},{"sourceId":11910577,"sourceType":"datasetVersion","datasetId":7487776},{"sourceId":12038896,"sourceType":"datasetVersion","datasetId":7377931},{"sourceId":12318530,"sourceType":"datasetVersion","datasetId":7762232}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"RUN_TRAIN = True # bfloat16 or float32 recommended\nRUN_VALID = True\nRUN_TEST  = False\n\nimport torch\nif not torch.cuda.is_available() or torch.cuda.device_count() < 2:\n    raise RuntimeError(\"Requires >= 2 GPUs with CUDA enabled.\")\n\ntry: \n    import monai\nexcept: \n    !pip install --no-deps monai -q","metadata":{"_uuid":"96aa1c5d-de16-4f41-b238-6eef3608c2fc","_cell_guid":"1c0d7af7-7dd9-4353-b509-796694347026","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-30T15:42:42.209955Z","iopub.execute_input":"2025-06-30T15:42:42.210315Z","iopub.status.idle":"2025-06-30T15:42:57.363395Z","shell.execute_reply.started":"2025-06-30T15:42:42.210288Z","shell.execute_reply":"2025-06-30T15:42:57.362548Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwandb_key = user_secrets.get_secret(\"wandb_key\")","metadata":{"_uuid":"2c3916a6-ec61-4f94-95c8-15d23ceaf7f5","_cell_guid":"2cceae28-c746-440f-998a-ebdc9a480af8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-30T15:42:57.364435Z","iopub.execute_input":"2025-06-30T15:42:57.365049Z","iopub.status.idle":"2025-06-30T15:42:57.525603Z","shell.execute_reply.started":"2025-06-30T15:42:57.365019Z","shell.execute_reply":"2025-06-30T15:42:57.524990Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# ConvNeXt Baseline Notebook\n\nThis notebook builds on [ConvNeXt - Full Resolution Baseline](https://www.kaggle.com/code/brendanartley/convnext-full-resolution-baseline) notebook. \n","metadata":{"_uuid":"a2387e20-542d-4ceb-a93d-716b38c7dcd0","_cell_guid":"0eebceca-e4ad-4bea-ac2c-307862f2f0fa","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"%%writefile _cfg.py\n\nfrom types import SimpleNamespace\nimport torch\n\ncfg= SimpleNamespace()\ncfg.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ncfg.local_rank = 0\ncfg.seed = 123\ncfg.subsample = None # None=full dataset, int32=first n rows with respect to \"dataset\"\ncfg.subsample_fraction = 0.3  # Use % of data from each dataset family, or None \n\ncfg.backbone = \"convnext_small.fb_in22k_ft_in1k\"\ncfg.ema = True\ncfg.ema_decay = 0.99\n\ncfg.epochs = 5\ncfg.batch_size = 32\ncfg.batch_size_val = 32\n\ncfg.early_stopping = {\"patience\": 3, \"streak\": 0}\ncfg.logging_steps = 100\n\ncfg.use_wandb = True\ncfg.wandb_project = \"seismic-inversion\"\n\n# ADDED NEW LINES:\ncfg.run_name = \"convnext_0.3_[-2]layer\"\n\ncfg.mixed_precision = False  # Set to True for A100, False for T4\ncfg.precision_dtype = torch.float32  # torch.float16 or torch.bfloat16\ncfg.use_grad_scaler = False  # Automatically set based on mixed_precision\n\ncfg.compile_model = True\n\n# GPU Logic: Auto-detect based on environment\ncfg.distributed = True  # Will be overridden by auto-detection\n\n# 72x72 vs Full Resolution Mode\ncfg.use_72x72_mode = False  # Set to True for 72x72 training, False for full resolution\ncfg.data_path_72x72 = \"/kaggle/input/openfwi-preprocessed-72x72/openfwi_72x72/\"\n\n# 256x72 unusual preprocessing \ncfg.use_smart_256x72_mode = False\n\ncfg.smoothness_loss = False\n\n# Augmentation settings\ncfg.use_augmentations = False  # Enable/disable training augmentations\ncfg.aug_noise_prob = 0.4      # Probability of noise augmentation\ncfg.aug_noise_level = 0.005    # Noise level as fraction of signal std\ncfg.aug_scale_prob = 0.3      # Probability of scale augmentation  \ncfg.aug_scale_range = (0.99, 1.01)  # Scale range (min, max)\n\n# TTA settings  \ncfg.enhanced_tta = False       # Enhanced TTA vs basic flip TTA\ncfg.tta_noise_level = 0.005   # TTA noise level as fraction of signal std\ncfg.tta_scale_values = [0.98, 1.02]  # Fixed scale values for TTA\n\n# Decoder layer selection\ncfg.decoder_layer_index = -2  # -1 = last layer (x[-1]), -2 = second-to-last (x[-2]), etc.\ncfg.use_feature_fusion = False  # True = fuse multiple layers, False = single layer\n\n# Feature fusion settings (only used if use_feature_fusion = True)\ncfg.fusion_layers = [-1, -2]   # Which layers to combine\ncfg.fusion_weights = [0.7, 0.3]  # Weights for each layer (must match fusion_layers length)\n\n","metadata":{"_uuid":"42b69892-66bd-423a-86ac-9ddf9d36183a","_cell_guid":"39ce23bb-352d-4a49-a094-885cdf9e7abd","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-30T16:06:39.821902Z","iopub.execute_input":"2025-06-30T16:06:39.822684Z","iopub.status.idle":"2025-06-30T16:06:39.829379Z","shell.execute_reply.started":"2025-06-30T16:06:39.822647Z","shell.execute_reply":"2025-06-30T16:06:39.828803Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Overwriting _cfg.py\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"### Dataset\n \n\nThe input dataset changes to [openfwi_float16_1](https://www.kaggle.com/datasets/egortrushin/open-wfi-1) and [openfwi_float16_2](https://www.kaggle.com/datasets/egortrushin/open-wfi-2) datasets.","metadata":{"_uuid":"ef50c9c2-64d1-48a6-b513-3c075c8bea05","_cell_guid":"1af18415-e9f6-4943-b1ef-2f7716000e6d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"%%writefile _dataset.py\n\nimport os\nimport glob\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport torch\n\nfrom _cfg import cfg\n\nimport torch\nimport torch.nn.functional as F\n\ndef smart_preprocess_256x72(x):\n    \"\"\"\n    Smart preprocessing: Early arrivals focus + minimal padding to 72\n    \n    Args:\n        x: numpy array, shape (sources, time, receivers) - single sample\n    \n    Returns:\n        numpy array, shape (sources, 256, 72)\n    \"\"\"\n    \n    x_tensor = torch.from_numpy(x).float()\n    \n    \n    time_samples = x_tensor.shape[1]\n    crop_samples = min(512, time_samples)\n    x_cropped = x_tensor[:, :crop_samples, :]  # (sources, 512, receivers)\n    \n    \n    sources, _, receivers = x_cropped.shape\n    x_reshaped = x_cropped.unsqueeze(1)  \n    \n    # Downsample to 256×70 first (preserve original spatial resolution)\n    x_downsampled = F.interpolate(x_reshaped, size=(256, 70), mode='area')\n    x_downsampled = x_downsampled.squeeze(1)  \n    \n    x_padded = F.pad(x_downsampled, (1, 1, 0, 0), mode='replicate')  \n        \n    return x_padded.numpy().astype(np.float16)\n\ndef inputs_files_to_output_files(input_files):\n    \"\"\"Convert input file paths to output file paths\"\"\"\n    return [\n        f.replace('/seis', '/vel').replace('/data', '/model')\n        for f in input_files\n    ]\n\ndef get_72x72_data_files(data_path):\n    \"\"\"Get data files for 72x72 mode\"\"\"\n    # All filenames\n    all_inputs = [\n        f for f in glob.glob(data_path + \"/*/*.npy\")\n        if ('/seis' in f) or ('/data' in f)\n    ]\n    all_outputs = inputs_files_to_output_files(all_inputs)\n    assert all([x != y for x,y in zip(all_inputs, all_outputs)])\n\n    # Validation filenames (same split as HGNet)\n    val_fpaths= [\n        'CurveFault_A/seis2_1_0.npy', 'CurveFault_A/seis2_1_1.npy', \n        'CurveFault_B/seis6_1_0.npy', 'CurveFault_B/seis6_1_1.npy', \n        'CurveVel_A/data1.npy', 'CurveVel_A/data10.npy', \n        'CurveVel_B/data1.npy', 'CurveVel_B/data10.npy', \n        'FlatFault_A/seis2_1_0.npy', 'FlatFault_A/seis2_1_1.npy', \n        'FlatFault_B/seis6_1_0.npy', 'FlatFault_B/seis6_1_1.npy', \n        'FlatVel_A/data1.npy', 'FlatVel_A/data10.npy', \n        'FlatVel_B/data1.npy', 'FlatVel_B/data10.npy', \n        'Style_A/data1.npy', 'Style_A/data10.npy', \n        'Style_B/data1.npy', 'Style_B/data10.npy',\n    ]\n\n    train_inputs, train_outputs = [], []\n    valid_inputs, valid_outputs = [], []\n\n    # Iterate and split files\n    for a, b in zip(all_inputs, all_outputs):\n        to_val = False\n        \n        for c in val_fpaths:\n            if c in a:\n                to_val = True\n\n        if to_val:\n            valid_inputs.append(a)\n            valid_outputs.append(b)\n        else:\n            train_inputs.append(a)\n            train_outputs.append(b)\n\n    return train_inputs, train_outputs, valid_inputs, valid_outputs\n\nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(\n        self, \n        cfg,\n        mode = \"train\", \n    ):\n        self.cfg = cfg\n        self.mode = mode\n        \n        if cfg.use_72x72_mode:\n            self.data, self.labels, self.records = self.load_72x72_metadata()\n        else:\n            self.data, self.labels, self.records = self.load_metadata()\n\n    def load_metadata(self):\n        \"\"\"Original full resolution data loading\"\"\"\n        # Select rows\n        df= pd.read_csv(\"/kaggle/input/openfwi-preprocessed-72x72/folds.csv\")\n        \n        if hasattr(self.cfg, 'subsample_fraction') and self.cfg.subsample_fraction is not None:\n            # Sample by fraction within each dataset family\n            df = df.groupby([\"dataset\", \"fold\"]).apply(\n                lambda x: x.sample(frac=self.cfg.subsample_fraction, random_state=self.cfg.seed)\n            ).reset_index(drop=True)\n        elif self.cfg.subsample is not None:\n            # Original fixed-number sampling\n            df= df.groupby([\"dataset\", \"fold\"]).head(self.cfg.subsample)\n\n        if self.mode == \"train\":\n            df= df[df[\"fold\"] != 0]\n        else:\n            df= df[df[\"fold\"] == 0]\n\n        data = []\n        labels = []\n        records = []\n        mmap_mode = \"r\"\n\n        for idx, row in tqdm(df.iterrows(), total=len(df), disable=self.cfg.local_rank != 0):\n            row= row.to_dict()\n\n           \n            # Original full dataset paths\n            p1 = os.path.join(\"/kaggle/input/open-wfi-1/openfwi_float16_1/\", row[\"data_fpath\"])\n            p2 = os.path.join(\"/kaggle/input/open-wfi-1/openfwi_float16_1/\", row[\"data_fpath\"].split(\"/\")[0], \"*\", row[\"data_fpath\"].split(\"/\")[-1])\n            p3 = os.path.join(\"/kaggle/input/open-wfi-2/openfwi_float16_2/\", row[\"data_fpath\"])\n            p4 = os.path.join(\"/kaggle/input/open-wfi-2/openfwi_float16_2/\", row[\"data_fpath\"].split(\"/\")[0], \"*\", row[\"data_fpath\"].split(\"/\")[-1])\n            farr = glob.glob(p1) + glob.glob(p2) + glob.glob(p3) + glob.glob(p4)\n        \n            # Map to lbl fpath\n            farr= farr[0]\n            flbl= farr.replace('seis', 'vel').replace('data', 'model')\n            \n            # Load\n            arr= np.load(farr, mmap_mode=mmap_mode)\n            lbl= np.load(flbl, mmap_mode=mmap_mode)\n\n            # Append\n            data.append(arr)\n            labels.append(lbl)\n            records.append(row[\"dataset\"])\n\n        return data, labels, records\n\n    def load_72x72_metadata(self):\n        \"\"\"Load 72x72 preprocessed data\"\"\"\n        train_inputs, train_outputs, valid_inputs, valid_outputs = get_72x72_data_files(self.cfg.data_path_72x72)\n        \n        if self.mode == \"train\":\n            input_files = train_inputs\n            output_files = train_outputs\n        else:\n            input_files = valid_inputs\n            output_files = valid_outputs\n\n        data = []\n        labels = []\n        records = []\n\n        for input_file, output_file in tqdm(zip(input_files, output_files), total=len(input_files), disable=self.cfg.local_rank != 0):\n            # Load data\n            arr = np.load(input_file, mmap_mode='r')\n            lbl = np.load(output_file, mmap_mode='r')\n            \n            # Extract dataset name from path\n            dataset_name = input_file.split(\"/\")[-2]\n            \n            data.append(arr)\n            labels.append(lbl)\n            records.append(dataset_name)\n\n        return data, labels, records\n\n    def __getitem__(self, idx):\n        row_idx = idx // 500\n        col_idx = idx % 500\n    \n        d = self.records[row_idx]\n        x = self.data[row_idx][col_idx, ...]\n        y = self.labels[row_idx][col_idx, ...]\n    \n        # Augmentations - before preprocessing\n        if self.mode == \"train\":\n            # Flip augmentation (50% chance)\n            if np.random.random() < 0.5:\n                if self.cfg.use_72x72_mode:\n                    x = x[::-1, :, ::-1]\n                    y = y[:, ::-1]\n                elif self.cfg.use_smart_256x72_mode:\n                    x = x[::-1, :, ::-1]\n                    y = y[:, ::-1]\n                else:\n                    x = x[::-1, :, ::-1]\n                    y = y[..., ::-1]\n                    \n            if self.cfg.use_augmentations:\n            \n                # Configurable noise augmentation\n                if np.random.random() < self.cfg.aug_noise_prob:\n                    noise_level = self.cfg.aug_noise_level * np.std(x)\n                    noise = np.random.normal(0, noise_level, x.shape).astype(x.dtype)\n                    x = x + noise\n                    \n                    # Safety check for NaN/inf\n                    if np.isnan(x).any() or np.isinf(x).any():\n                        print(f\"Warning: NaN/inf detected in noise augmentation, reverting\")\n                        x = x - noise\n                \n                # Configurable scale augmentation\n                if np.random.random() < self.cfg.aug_scale_prob:\n                    scale = np.random.uniform(self.cfg.aug_scale_range[0], self.cfg.aug_scale_range[1])\n                    x = x * scale\n                    \n                    # Safety check for NaN/inf\n                    if np.isnan(x).any() or np.isinf(x).any():\n                        print(f\"Warning: NaN/inf detected in scale augmentation, reverting\")\n                        x = x / scale\n    \n        x = x.copy()\n        y = y.copy()\n    \n        if self.cfg.use_smart_256x72_mode:\n            x = smart_preprocess_256x72(x)\n        \n        elif self.cfg.use_72x72_mode:\n            pass\n    \n        if not self.cfg.mixed_precision:\n            x = x.astype(np.float32)\n            y = y.astype(np.float32)\n        \n        return x, y\n    \n    def __len__(self, ):\n        return len(self.records) * 500","metadata":{"_uuid":"f4f7063f-0324-416f-b2ff-933cebc7dc3d","_cell_guid":"29fe412c-4e77-4a15-a33a-5813ea2526c7","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-30T16:06:42.045405Z","iopub.execute_input":"2025-06-30T16:06:42.045718Z","iopub.status.idle":"2025-06-30T16:06:42.054487Z","shell.execute_reply.started":"2025-06-30T16:06:42.045699Z","shell.execute_reply":"2025-06-30T16:06:42.053642Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Overwriting _dataset.py\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"# Model\n\nThe model uses the `ConvNeXt` backbone from timm. See more info on this backbone [here](https://huggingface.co/timm/convnext_small.fb_in22k_ft_in1k) and the original paper [here](https://arxiv.org/abs/2201.03545). We modify the stem to aggressively downsample the height and we replace normalization layers with `InstanceNorm2d`.\n\n### Encoder\n\nFor the unet, we typically want the encoder to downsample by a factor of 2x at each stage. This works best when the input is square so that we can use as little padding as possible. In the original notebook, we did this by interpolating the input data. This worked okay, but we lost a lot of detail as a result. Here we rely on the stem to downsample using convolutions. See the `update_stem()` function for more details. \n\n#### Normalization\n\nMost CNNs use `BatchNorm2D`, which relies on batch statistics when computing normalization. However, replacing this with a batch-independent normalization layer like `InstanceNorm2D` or `LayerNorm` can improve convergence speed and stabilize validation performance. ConvNeXt uses `LayerNorm` by default, but we use `InstanceNorm2D` instead.\n\nSince normalization is now independent of batch statistics, smaller batch sizes can be used without a drop in performance. For example, a batch-size of 16 uses ~9GB of vRAM during training.\n\n### Decoder\n\nThe decoder is mostly the same. One small change is that we did not use intermediate convolutions here.","metadata":{"_uuid":"8c842a59-b9f9-4f35-8c7d-710659df4d79","_cell_guid":"e3f03ef0-083a-45d0-b0a2-cdec62bfe7e5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"%%writefile _model.py\nfrom copy import deepcopy\nfrom types import MethodType\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport timm\nfrom timm.models.convnext import ConvNeXtBlock\n\nfrom monai.networks.blocks import UpSample, SubpixelUpsample\n\n####################\n## EMA + Ensemble ##\n####################\n\nclass ModelEMA(nn.Module):\n    def __init__(self, model, decay=0.99, device=None):\n        super().__init__()\n        self.module = deepcopy(model)\n        self.module.eval()\n        self.decay = decay\n        self.device = device\n        if self.device is not None:\n            self.module.to(device=device)\n\n    def _update(self, model, update_fn):\n        with torch.no_grad():\n            for ema_v, model_v in zip(self.module.state_dict().values(), model.state_dict().values()):\n                if self.device is not None:\n                    model_v = model_v.to(device=self.device)\n                ema_v.copy_(update_fn(ema_v, model_v))\n\n    def update(self, model):\n        self._update(model, update_fn=lambda e, m: self.decay * e + (1. - self.decay) * m)\n\n    def set(self, model):\n        self._update(model, update_fn=lambda e, m: m)\n\n\nclass EnsembleModel(nn.Module):\n    def __init__(self, models):\n        super().__init__()\n        self.models = nn.ModuleList(models).eval()\n\n    def forward(self, x):\n        output = None\n        \n        for m in self.models:\n            logits= m(x)\n            \n            if output is None:\n                output = logits\n            else:\n                output += logits\n                \n        output /= len(self.models)\n        return output\n        \n\n#############\n## Decoder ##\n#############\n\nclass ConvBnAct2d(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        padding: int = 0,\n        stride: int = 1,\n        norm_layer: nn.Module = nn.Identity,\n        act_layer: nn.Module = nn.ReLU,\n    ):\n        super().__init__()\n\n        self.conv= nn.Conv2d(\n            in_channels, \n            out_channels,\n            kernel_size,\n            stride=stride, \n            padding=padding, \n            bias=False,\n        )\n        self.norm = norm_layer(out_channels) if norm_layer != nn.Identity else nn.Identity()\n        self.act= act_layer(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.norm(x)\n        x = self.act(x)\n        return x\n\n\nclass SCSEModule2d(nn.Module):\n    def __init__(self, in_channels, reduction=16):\n        super().__init__()\n        self.cSE = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels, in_channels // reduction, 1),\n            nn.Tanh(),\n            nn.Conv2d(in_channels // reduction, in_channels, 1),\n            nn.Sigmoid(),\n        )\n        self.sSE = nn.Sequential(\n            nn.Conv2d(in_channels, 1, 1), \n            nn.Sigmoid(),\n            )\n\n    def forward(self, x):\n        return x * self.cSE(x) + x * self.sSE(x)\n\nclass Attention2d(nn.Module):\n    def __init__(self, name, **params):\n        super().__init__()\n        if name is None:\n            self.attention = nn.Identity(**params)\n        elif name == \"scse\":\n            self.attention = SCSEModule2d(**params)\n        else:\n            raise ValueError(\"Attention {} is not implemented\".format(name))\n\n    def forward(self, x):\n        return self.attention(x)\n\nclass DecoderBlock2d(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        skip_channels,\n        out_channels,\n        norm_layer: nn.Module = nn.Identity,\n        attention_type: str = None,\n        intermediate_conv: bool = False,\n        upsample_mode: str = \"deconv\",\n        scale_factor: int = 2,\n    ):\n        super().__init__()\n\n        # Upsample block\n        if upsample_mode == \"pixelshuffle\":\n            self.upsample= SubpixelUpsample(\n                spatial_dims= 2,\n                in_channels= in_channels,\n                scale_factor= scale_factor,\n            )\n        else:\n            self.upsample = UpSample(\n                spatial_dims= 2,\n                in_channels= in_channels,\n                out_channels= in_channels,\n                scale_factor= scale_factor,\n                mode= upsample_mode,\n            )\n\n        if intermediate_conv:\n            k= 3\n            c= skip_channels if skip_channels != 0 else in_channels\n            self.intermediate_conv = nn.Sequential(\n                ConvBnAct2d(c, c, k, k//2),\n                ConvBnAct2d(c, c, k, k//2),\n                )\n        else:\n            self.intermediate_conv= None\n\n        self.attention1 = Attention2d(\n            name= attention_type, \n            in_channels= in_channels + skip_channels,\n            )\n\n        self.conv1 = ConvBnAct2d(\n            in_channels + skip_channels,\n            out_channels,\n            kernel_size= 3,\n            padding= 1,\n            norm_layer= norm_layer,\n        )\n\n        self.conv2 = ConvBnAct2d(\n            out_channels,\n            out_channels,\n            kernel_size= 3,\n            padding= 1,\n            norm_layer= norm_layer,\n        )\n        self.attention2 = Attention2d(\n            name= attention_type, \n            in_channels= out_channels,\n            )\n\n    def forward(self, x, skip=None):\n        x = self.upsample(x)\n\n        if self.intermediate_conv is not None:\n            if skip is not None:\n                skip = self.intermediate_conv(skip)\n            else:\n                x = self.intermediate_conv(x)\n\n        if skip is not None:\n            x = torch.cat([x, skip], dim=1)\n            x = self.attention1(x)\n\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.attention2(x)\n        return x\n\n\nclass UnetDecoder2d(nn.Module):\n    \"\"\"\n    Unet decoder.\n    Source: https://arxiv.org/abs/1505.04597\n    \"\"\"\n    def __init__(\n        self,\n        encoder_channels: tuple[int],\n        skip_channels: tuple[int] = None,\n        decoder_channels: tuple = (256, 128, 64, 32),\n        scale_factors: tuple = (2,2,2,2),\n        norm_layer: nn.Module = nn.Identity,\n        attention_type: str = None,\n        intermediate_conv: bool = False,\n        upsample_mode: str = \"deconv\",\n    ):\n        super().__init__()\n        \n        if len(encoder_channels) == 4:\n            decoder_channels= decoder_channels[1:]\n        self.decoder_channels= decoder_channels\n        \n        if skip_channels is None:\n            skip_channels= list(encoder_channels[1:]) + [0]\n        \n        # STORE skip_channels for use in forward method\n        self.skip_channels = skip_channels\n        \n        # Build decoder blocks\n        in_channels= [encoder_channels[0]] + list(decoder_channels[:-1])\n        self.blocks = nn.ModuleList()\n        for i, (ic, sc, dc) in enumerate(zip(in_channels, skip_channels, decoder_channels)):\n            self.blocks.append(\n                DecoderBlock2d(\n                    ic, sc, dc, \n                    norm_layer= norm_layer,\n                    attention_type= attention_type,\n                    intermediate_conv= intermediate_conv,\n                    upsample_mode= upsample_mode,\n                    scale_factor= scale_factors[i],\n                    )\n            )\n    \n    def forward(self, feats: list[torch.Tensor]):\n        res= [feats[0]]\n        feats= feats[1:]\n        \n        for i, b in enumerate(self.blocks):\n            if i < len(self.skip_channels) and self.skip_channels[i] > 0:\n                skip = feats[i] if i < len(feats) else None\n            else:\n                skip = None  # Force skip to None when skip_channels=0\n                \n            res.append(\n                b(res[-1], skip=skip),\n            )\n            \n        return res\n\nclass SegmentationHead2d(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        scale_factor: tuple[int] = (2,2),\n        kernel_size: int = 3,\n        mode: str = \"nontrainable\",\n    ):\n        super().__init__()\n        self.conv= nn.Conv2d(\n            in_channels, out_channels, kernel_size= kernel_size,\n            padding= kernel_size//2\n        )\n        self.upsample = UpSample(\n            spatial_dims= 2,\n            in_channels= out_channels,\n            out_channels= out_channels,\n            scale_factor= scale_factor,\n            mode= mode,\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.upsample(x)\n        return x\n\n#################################\n## PERFECT SEGMENTATION HEAD  ##\n#################################\n\nclass PerfectSegmentationHead2d(nn.Module):\n    \"\"\"\n    Perfect segmentation head: 64×64 → exactly 70×70\n    Uses learnable upsampling for precise output size\n    \"\"\"\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        \n        # Feature processing\n        self.conv1 = nn.Conv2d(in_channels, in_channels//2, 3, padding=1)\n        self.norm1 = nn.InstanceNorm2d(in_channels//2, affine=True)\n        self.act1 = nn.GELU()\n        \n        self.conv2 = nn.Conv2d(in_channels//2, out_channels, 3, padding=1)\n        \n        # Using ConvTranspose2d with exact kernel/padding for 70×70\n        self.upsample = nn.ConvTranspose2d(\n            out_channels, out_channels,\n            kernel_size=7, stride=1, padding=0  \n        )\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.norm1(x)\n        x = self.act1(x)\n        x = self.conv2(x)\n        \n        x = self.upsample(x)  \n        \n        return x\n        \n\n#############\n## Encoder ##\n#############\n\ndef _convnext_block_forward(self, x):\n    shortcut = x\n    x = self.conv_dw(x)\n\n    if self.use_conv_mlp:\n        x = self.norm(x)\n        x = self.mlp(x)\n    else:\n        x = self.norm(x)\n        x = x.permute(0, 2, 3, 1)\n        x = x.contiguous()\n        x = self.mlp(x)\n        x = x.permute(0, 3, 1, 2)\n        x = x.contiguous()\n\n    if self.gamma is not None:\n        x = x * self.gamma.reshape(1, -1, 1, 1)\n\n    x = self.drop_path(x) + self.shortcut(shortcut)\n    return x\n\n\nclass Net(nn.Module):\n    \n    def __init__(\n        self,\n        backbone: str,\n        pretrained: bool = True,\n        use_72x72_mode: bool = False,\n        use_smart_256x72_mode: bool = False,\n    ):\n        super().__init__()\n        \n        self.use_72x72_mode = use_72x72_mode\n        self.use_smart_256x72_mode = use_smart_256x72_mode\n        \n        # Encoder\n        self.backbone= timm.create_model(\n            backbone,\n            in_chans= 5,\n            pretrained= pretrained,\n            features_only= True,\n            drop_path_rate=0.0,\n            )\n        ecs= [_[\"num_chs\"] for _ in self.backbone.feature_info][::-1]\n\n        \n        # Decoder - DIFFERENT for each mode\n        if use_smart_256x72_mode:\n            self.decoder= UnetDecoder2d(\n                encoder_channels= ecs,\n                scale_factors= (2, 2, 2, 2),  \n            )\n            \n            self.seg_head = PerfectSegmentationHead2d(\n                in_channels=self.decoder.decoder_channels[-1],\n                out_channels=1,\n            )\n            \n            print(\"[Properly Engineered 256×72] Using PerfectSegmentationHead2d for exact 70×70 output\")\n            \n        elif use_72x72_mode:\n            # Modified decoder with different scale factors for 72x72\n            # The fundamental issue is that 72×72 is too small for this U-Net architecture. \n            # We're trying to fit a small input into a model designed for large images. \n            # For 72×72, a simpler decoder without skip connections was tried, but unsuccessful.\n            self.decoder= UnetDecoder2d(\n                encoder_channels= ecs,\n                skip_channels= [0, 0, 0, 0],\n                scale_factors= (3, 3, 2, 2),  \n            )\n            \n            self.seg_head= SegmentationHead2d(\n                in_channels= self.decoder.decoder_channels[-1],\n                out_channels= 1,\n                scale_factor= 1,\n            )\n        else:\n            # Original decoder for full resolution\n            self.decoder= UnetDecoder2d(\n                encoder_channels= ecs,\n                scale_factors= (2, 2, 2, 2),  \n            )\n    \n            self.seg_head= SegmentationHead2d(\n                in_channels= self.decoder.decoder_channels[-1],\n                out_channels= 1,\n                scale_factor= 1,\n            )\n\n        \n        \n        # Stem modifications\n        if use_smart_256x72_mode:\n            self._update_stem_256x72_properly_engineered(backbone)\n        elif use_72x72_mode:\n            self._update_stem_72x72(backbone)\n        else:\n            self._update_stem_full_res(backbone)\n        \n        self.replace_activations(self.backbone, log=True)\n        self.replace_norms(self.backbone, log=True)\n        self.replace_forwards(self.backbone, log=True)\n\n        decoder_channels = self.decoder.decoder_channels\n        self.channel_adapters = nn.ModuleDict({\n            '-1': nn.Identity(),\n            '-2': nn.Conv2d(decoder_channels[-2], decoder_channels[-1], 1) if len(decoder_channels) >= 2 else nn.Identity(),\n            '-3': nn.Conv2d(decoder_channels[-3], decoder_channels[-1], 1) if len(decoder_channels) >= 3 else nn.Identity(),\n            '-4': nn.Conv2d(decoder_channels[-4], decoder_channels[-1], 1) if len(decoder_channels) >= 4 else nn.Identity(),\n        })\n        \n    \n    def _update_stem_256x72_properly_engineered(self, backbone):\n        \"\"\"\n        PROPERLY ENGINEERED: 256×72 → 32×32 for perfect 70×70 output\n        Works backwards from target to ensure exact dimensions\n        \"\"\"\n        if backbone.startswith(\"convnext\"):\n            original_stem = self.backbone.stem_0\n            original_weight = original_stem.weight\n            original_bias = original_stem.bias\n            out_channels = original_weight.shape[0]\n            \n            # Calculate exact padding needed\n            # Target progression: 256×72 → 280×96 → 70×48 → 33×48 → 32×40 → 32×32\n            \n            conv1 = nn.Conv2d(\n                5, out_channels, \n                kernel_size=6, stride=(4, 2), padding=1\n            )\n            # 280×96 → 70×48\n            \n            conv2 = nn.Conv2d(\n                out_channels, out_channels,\n                kernel_size=(5, 3), stride=(2, 1), padding=(0, 1)\n            )\n            # 70×48 → 33×48\n            \n            conv3 = nn.Conv2d(\n                out_channels, out_channels, \n                kernel_size=(2, 9), stride=(1, 1), padding=0\n            )\n            # 33×48 → 32×40\n            \n            self.backbone.stem_0 = nn.Sequential(\n                nn.ReflectionPad2d((12, 12, 12, 12)),  \n                \n                conv1,  # 280×96 → 70×48\n                nn.LayerNorm([out_channels, 70, 48]),\n                nn.GELU(),\n                \n                conv2,  # 70×48 → 33×48  \n                nn.LayerNorm([out_channels, 33, 48]),\n                nn.GELU(),\n                \n                conv3,  # 33×48 → 32×40\n                nn.LayerNorm([out_channels, 32, 40]),\n                nn.GELU(),\n                \n                nn.AdaptiveAvgPool2d((32, 32))  # 32×40 → 32×32\n            )\n            \n            with torch.no_grad():\n                conv1.weight.data.normal_(0, 0.02)\n                conv1.bias.data.zero_()\n\n                if original_weight.shape[2] <= 6 and original_weight.shape[3] <= 6:\n                    conv1.weight[:, :, :original_weight.shape[2], :original_weight.shape[3]].copy_(original_weight)\n                    conv1.bias.copy_(original_bias)\n                    \n                conv2.weight.data.normal_(0, 0.02)\n                conv2.bias.data.zero_()\n                conv3.weight.data.normal_(0, 0.02) \n                conv3.bias.data.zero_()\n            \n            print(f\"[PROPERLY ENGINEERED] Complete stem flow:\")\n            print(f\"  256×72 → ReflectionPad(280×96) → Conv1(70×48) → Conv2(33×48) → Conv3(32×40) → Pool(32×32)\")\n            print(f\"  RESULT: Perfect 32×32 → Perfect skip connections → Perfect 70×70 output!\")\n            \n        else:\n            raise ValueError(\"Properly engineered stem not implemented for this backbone.\")\n\n    def _update_stem_72x72(self, backbone):\n        \"\"\"Proper stem modifications for 72x72 mode\"\"\"\n        if backbone.startswith(\"convnext\"):\n            self.backbone.stem_0.stride = (2, 2)  # 72÷2 = 36\n            self.backbone.stem_0.padding = (1, 1)  # Standard padding\n        else:\n            raise ValueError(\"Custom striding not implemented for 72x72 mode.\")\n\n    def _update_stem_full_res(self, backbone):\n        \"\"\"Original aggressive stem modifications for full resolution - EXACTLY AS ORIGINAL\"\"\"\n        if backbone.startswith(\"convnext\"):\n\n            # Update stride\n            self.backbone.stem_0.stride = (4, 1)\n            self.backbone.stem_0.padding = (0, 2)\n\n            # Duplicate stem layer (to downsample height)\n            with torch.no_grad():\n                w = self.backbone.stem_0.weight\n                new_conv= nn.Conv2d(w.shape[0], w.shape[0], kernel_size=(4, 4), stride=(4, 1), padding=(0, 1))\n                new_conv.weight.copy_(w.repeat(1, (128//w.shape[1])+1, 1, 1)[:, :new_conv.weight.shape[1], :, :])\n                new_conv.bias.copy_(self.backbone.stem_0.bias)\n\n            self.backbone.stem_0= nn.Sequential(\n                nn.ReflectionPad2d((1,1,80,80)),\n                self.backbone.stem_0,\n                new_conv,\n            )\n\n        else:\n            raise ValueError(\"Custom striding not implemented.\")\n        pass\n\n    def replace_activations(self, module, log=False):\n        if log and self.use_smart_256x72_mode:\n            print(f\"[256×72 Mode] Replacing all activations with GELU...\")\n        elif log and self.use_72x72_mode:\n            print(f\"[72x72 Mode] Replacing all activations with GELU...\")\n        elif log:\n            print(f\"[Full Res Mode] Replacing all activations with GELU...\")\n        \n        for name, child in module.named_children():\n            if isinstance(child, (\n                nn.ReLU, nn.LeakyReLU, nn.Mish, nn.Sigmoid, \n                nn.Tanh, nn.Softmax, nn.Hardtanh, nn.ELU, \n                nn.SELU, nn.PReLU, nn.CELU, nn.GELU, nn.SiLU,\n            )):\n                setattr(module, name, nn.GELU())\n            else:\n                self.replace_activations(child)\n\n    def replace_norms(self, mod, log=False):\n        if log and self.use_smart_256x72_mode:\n            print(f\"[256×72 Mode] Replacing all norms with InstanceNorm...\")\n        elif log and self.use_72x72_mode:\n            print(f\"[72x72 Mode] Replacing all norms with InstanceNorm...\")\n        elif log:\n            print(f\"[Full Res Mode] Replacing all norms with InstanceNorm...\")\n            \n        for name, c in mod.named_children():\n            n_feats= None\n            if isinstance(c, (nn.BatchNorm2d, nn.InstanceNorm2d)):\n                n_feats= c.num_features\n            elif isinstance(c, (nn.GroupNorm,)):\n                n_feats= c.num_channels\n            elif isinstance(c, (nn.LayerNorm,)):\n                n_feats= c.normalized_shape[0]\n\n            if n_feats is not None:\n                new = nn.InstanceNorm2d(\n                    n_feats,\n                    affine=True,\n                    )\n                setattr(mod, name, new)\n            else:\n                self.replace_norms(c)\n\n    def replace_forwards(self, mod, log=False):\n        if log and self.use_smart_256x72_mode:\n            print(f\"[256×72 Mode] Replacing forward functions...\")\n        elif log and self.use_72x72_mode:\n            print(f\"[72x72 Mode] Replacing forward functions...\")\n        elif log:\n            print(f\"[Full Res Mode] Replacing forward functions...\")\n            \n        for name, c in mod.named_children():\n            if isinstance(c, ConvNeXtBlock):\n                c.forward = MethodType(_convnext_block_forward, c)\n            else:\n                self.replace_forwards(c)\n        \n\n    def get_decoder_features(self, decoder_outputs):\n        \"\"\"Get features from decoder based on config\"\"\"\n        from _cfg import cfg\n        \n        if cfg.use_feature_fusion:\n            # Use learnable weights if available, otherwise use config weights\n            if hasattr(self, 'fusion_weights'):\n                weights = torch.softmax(self.fusion_weights, dim=0)\n            else:\n                weights = cfg.fusion_weights\n            \n            target_size = decoder_outputs[-1].shape[-2:]\n            fused_features = None\n            \n            for i, layer_idx in enumerate(cfg.fusion_layers):\n                features = decoder_outputs[layer_idx]\n                \n                # Channel adaptation\n                adapter_key = str(layer_idx)\n                if adapter_key in self.channel_adapters:\n                    features = self.channel_adapters[adapter_key](features)\n                \n                # Spatial interpolation if needed\n                if features.shape[-2:] != target_size:\n                    features = F.interpolate(features, size=target_size, mode='bilinear', align_corners=False)\n                \n                weight = weights[i] if hasattr(self, 'fusion_weights') else weights[i]\n                \n                if fused_features is None:\n                    fused_features = weight * features\n                else:\n                    fused_features += weight * features\n            \n            return fused_features\n        else:\n            # Single layer selection\n            features = decoder_outputs[cfg.decoder_layer_index]\n            \n            # Channel adaptation\n            adapter_key = str(cfg.decoder_layer_index)\n            if adapter_key in self.channel_adapters:\n                features = self.channel_adapters[adapter_key](features)\n            \n            # Spatial interpolation if needed\n            target_size = decoder_outputs[-1].shape[-2:]\n            if features.shape[-2:] != target_size:\n                features = F.interpolate(features, size=target_size, mode='bilinear', align_corners=False)\n            \n            return features\n\n    def proc_flip(self, x_in):\n        x_in= torch.flip(x_in, dims=[-3, -1])\n        x= self.backbone(x_in)\n        x= x[::-1]\n    \n        # Decoder\n        x= self.decoder(x)\n        features_to_use = self.get_decoder_features(x)\n        x_seg= self.seg_head(features_to_use)\n        \n        # NO CROPPING for 256×72 mode - perfect 70×70 output!\n        if not self.use_smart_256x72_mode:\n            x_seg= x_seg[..., 1:-1, 1:-1]  # Only crop for other modes\n        \n        x_seg= torch.flip(x_seg, dims=[-1])\n        x_seg= x_seg * 1500 + 3000\n        return x_seg\n    \n    def forward(self, batch):\n        x= batch\n        \n        # Encoder\n        x_in = x\n        x= self.backbone(x)\n        x= x[::-1]\n    \n        # Decoder\n        x= self.decoder(x)\n        features_to_use = self.get_decoder_features(x)\n        x_seg= self.seg_head(features_to_use)\n        \n        if not self.use_smart_256x72_mode:\n            x_seg= x_seg[..., 1:-1, 1:-1]  # Only crop for other modes\n        \n        x_seg= x_seg * 1500 + 3000\n    \n        if self.training:\n            return x_seg\n        else:\n            from _cfg import cfg\n            \n            if not cfg.enhanced_tta:\n                # Basic TTA (your original)\n                p1 = self.proc_flip(x_in)\n                x_seg = torch.mean(torch.stack([x_seg, p1]), dim=0)\n                return x_seg\n            else:\n                # Enhanced TTA - collect multiple predictions\n                predictions = [x_seg]  # Original prediction\n                \n                p1 = self.proc_flip(x_in)\n                predictions.append(p1)\n                \n                # 2. Noise TTA variants\n                for _ in range(2):\n                    noise = torch.randn_like(x_in) * cfg.tta_noise_level * torch.std(x_in)\n                    x_noise = x_in + noise\n                    \n                    x_aug = self.backbone(x_noise)\n                    x_aug = x_aug[::-1]\n                    x_aug = self.decoder(x_aug)\n                    features_aug = self.get_decoder_features(x_aug)\n                    x_seg_aug = self.seg_head(features_aug)\n                    \n                    if not self.use_smart_256x72_mode:\n                        x_seg_aug = x_seg_aug[..., 1:-1, 1:-1]\n                    \n                    x_seg_aug = x_seg_aug * 1500 + 3000\n                    predictions.append(x_seg_aug)\n                \n                # 3. Scale TTA variants  \n                for scale in cfg.tta_scale_values:\n                    x_scaled = x_in * scale\n                    \n                    x_aug = self.backbone(x_scaled)\n                    x_aug = x_aug[::-1]\n                    x_aug = self.decoder(x_aug)\n                    features_aug = self.get_decoder_features(x_aug)\n                    x_seg_aug = self.seg_head(features_aug)\n                    \n                    if not self.use_smart_256x72_mode:\n                        x_seg_aug = x_seg_aug[..., 1:-1, 1:-1]\n                    \n                    x_seg_aug = x_seg_aug * 1500 + 3000\n                    predictions.append(x_seg_aug)\n                \n                x_seg = torch.mean(torch.stack(predictions), dim=0)\n                return x_seg","metadata":{"_uuid":"77216bb7-c35f-4807-8d57-e1cdc8546a88","_cell_guid":"5dc55040-ff57-4833-bff9-69a258808f0e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-30T16:06:43.068742Z","iopub.execute_input":"2025-06-30T16:06:43.069454Z","iopub.status.idle":"2025-06-30T16:06:43.085215Z","shell.execute_reply.started":"2025-06-30T16:06:43.069430Z","shell.execute_reply":"2025-06-30T16:06:43.084403Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Overwriting _model.py\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"### Utils\n\nSame as starter notebook.","metadata":{"_uuid":"64148277-390c-4f71-9b1e-b279dd1ba025","_cell_guid":"3c4b64cb-e447-4e9f-9c33-0685291046b1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"%%writefile _utils.py\n\nimport datetime\n\ndef format_time(elapsed):\n    elapsed_rounded = int(round((elapsed)))\n    return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"_uuid":"e71b484a-0797-420d-8e2f-50916e13a589","_cell_guid":"576e1448-bb2e-4bae-a242-1cd26301f7f8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-30T16:06:43.870364Z","iopub.execute_input":"2025-06-30T16:06:43.870954Z","iopub.status.idle":"2025-06-30T16:06:43.875724Z","shell.execute_reply.started":"2025-06-30T16:06:43.870928Z","shell.execute_reply":"2025-06-30T16:06:43.875102Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Overwriting _utils.py\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"# Train\n\nSame as starter notebook.","metadata":{"_uuid":"3a65d493-4130-4a9b-aece-2a85a912b301","_cell_guid":"1eb0311b-ded6-4128-95a3-06e33ccf5115","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"%%writefile _train.py\n\nimport os\nimport time \nimport random\nimport numpy as np\nfrom tqdm import tqdm\nfrom contextlib import nullcontext\n\nimport torch\nimport torch.nn as nn\nfrom torch.amp import autocast, GradScaler\n\n# Conditional imports for distributed\ntry:\n    import torch.distributed as dist\n    from torch.utils.data import DistributedSampler\n    from torch.nn.parallel import DistributedDataParallel\n    DIST_AVAILABLE = True\nexcept:\n    DIST_AVAILABLE = False\n\nfrom _cfg import cfg\nfrom _dataset import CustomDataset\nfrom _model import ModelEMA, Net\nfrom _utils import format_time\n\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\ntry:\n    import wandb\nexcept ImportError:\n    wandb = None\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    wandb_key = user_secrets.get_secret(\"wandb_key\")\n    if wandb is not None and wandb_key:\n        wandb.login(key=wandb_key)\nexcept:\n    pass\n\ndef set_seed(seed=1234):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\ndef setup(rank, world_size):\n    torch.cuda.set_device(rank)\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n    return\n\ndef cleanup():\n    dist.barrier()\n    dist.destroy_process_group()\n    return\n\n\n\ndef setup_gpu_mode(cfg):\n    \"\"\"Auto-detect GPU configuration and set distributed mode\"\"\"\n    import os\n    \n    # Check if running with torchrun (distributed)\n    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n        cfg.distributed = True\n        cfg.local_rank = int(os.environ[\"RANK\"])\n        cfg.world_size = int(os.environ[\"WORLD_SIZE\"])\n        print(f\"[Distributed] Rank: {cfg.local_rank}, World size: {cfg.world_size}\")\n    else:\n        cfg.distributed = False\n        cfg.local_rank = 0\n        cfg.world_size = 1\n        available_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0\n        if available_gpus == 0:\n            print(\"[Single GPU] No GPUs available, using CPU\")\n            cfg.device = torch.device(\"cpu\")\n        else:\n            print(f\"[Single GPU] Using 1 GPU out of {available_gpus} available\")\n            cfg.device = torch.device(\"cuda:0\")\n\ndef main_single_gpu(cfg):\n    \"\"\"Single GPU training\"\"\"\n    print(\"=\"*25)\n    print(\"Running on single GPU\")\n    print(\"=\"*25)\n    \n    # Print mode info\n    mode_str = \"72x72\" if cfg.use_72x72_mode else \"Full Resolution\"\n    print(f\"Training Mode: {mode_str}\")\n    \n    if cfg.use_wandb and wandb is not None:\n        run_name = f\"{cfg.run_name}_{cfg.seed}_single_gpu\"\n        if cfg.use_72x72_mode:\n            run_name += \"_72x72\"\n        wandb.init(\n            project=cfg.wandb_project,\n            config=vars(cfg),\n            name=run_name\n        )\n    \n    # Datasets\n    print(\"Loading data..\")\n    train_ds = CustomDataset(cfg=cfg, mode=\"train\")\n    train_dl = torch.utils.data.DataLoader(\n        train_ds, \n        batch_size=cfg.batch_size, \n        shuffle=True,\n        num_workers=4,\n        pin_memory=True,\n        persistent_workers=True,\n    )\n    \n    valid_ds = CustomDataset(cfg=cfg, mode=\"valid\")\n    valid_dl = torch.utils.data.DataLoader(\n        valid_ds, \n        batch_size=cfg.batch_size_val, \n        shuffle=False,\n        num_workers=4,\n        pin_memory=True,\n        persistent_workers=True,\n    )\n    \n    # Model\n    model = Net(backbone=cfg.backbone, use_72x72_mode=cfg.use_72x72_mode, use_smart_256x72_mode=cfg.use_smart_256x72_mode)\n\n    model = model.to(cfg.device)\n    \n    # Disable compilation for 72x72 mode\n    compile_enabled = cfg.compile_model\n    if compile_enabled:\n        model = torch.compile(model, mode='default')\n    \n    if cfg.ema:\n        print(\"Initializing EMA model..\")\n        ema_model = ModelEMA(model, decay=cfg.ema_decay, device=cfg.device)\n    else:\n        ema_model = None\n\n    \n    \n    # Training setup\n    criterion = nn.L1Loss()\n    \n    compile_enabled = cfg.compile_model\n    if compile_enabled:\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, fused=True)\n    else:\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    scheduler = CosineAnnealingLR(optimizer, T_max=cfg.epochs)\n\n    # Define autocast context\n    if cfg.mixed_precision:\n        autocast_context = autocast(device_type='cuda', dtype=cfg.precision_dtype)\n    else:\n        autocast_context = nullcontext()\n    \n    if cfg.use_grad_scaler and cfg.mixed_precision:\n        scaler = GradScaler()\n    else:\n        scaler = None\n    \n    print(\"=\"*25)\n    print(\"Starting training...\")\n    print(\"=\"*25)\n    \n    best_loss = 1_000_000\n    val_loss = 1_000_000\n    \n    for epoch in range(1, cfg.epochs+1):\n        tstart = time.time()\n        \n        # Train\n        model.train()\n        total_loss = []\n        \n        for i, (x, y) in enumerate(train_dl):\n            x = x.to(cfg.device)\n            y = y.to(cfg.device)\n            \n            with autocast_context:\n                logits = model(x)\n            \n            def smoothness_loss(pred_vel, alpha=0.1):\n                grad_x = torch.abs(pred_vel[:, :, :, 1:] - pred_vel[:, :, :, :-1])\n                grad_y = torch.abs(pred_vel[:, :, 1:, :] - pred_vel[:, :, :-1, :])\n                return alpha * (grad_x.mean() + grad_y.mean())\n            \n            if cfg.smoothness_loss:\n                mae_loss = criterion(logits, y)\n                smooth_loss = smoothness_loss(logits, alpha=0.1)\n                loss = mae_loss + smooth_loss\n            else:\n                loss = criterion(logits, y)\n            \n            if scaler is not None:\n                scaler.scale(loss).backward()\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n            else:\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n                optimizer.step()\n                optimizer.zero_grad()\n            \n            total_loss.append(loss.item())\n            \n            if ema_model is not None:\n                ema_model.update(model)\n            \n            if len(total_loss) >= cfg.logging_steps or i == 0:\n                train_loss = np.mean(total_loss)\n                total_loss = []\n                print(f\"Epoch {epoch}: Train MAE: {train_loss:.2f} Val MAE: {val_loss:.2f} \"\n                      f\"Time: {format_time(time.time() - tstart)} Step: {i+1}/{len(train_dl)}\")\n            \n            if cfg.use_wandb and wandb is not None and i % 50 == 0:\n                wandb.log({\n                    \"train_loss\": loss.item(),\n                    \"epoch\": epoch,\n                    \"step\": epoch * len(train_dl) + i\n                })\n        \n        # Validation\n        model.eval()\n        val_logits = []\n        val_targets = []\n        \n        with torch.no_grad():\n            for x, y in tqdm(valid_dl):\n                x = x.to(cfg.device)\n                y = y.to(cfg.device)\n                \n                with autocast_context:\n                    if ema_model is not None:\n                        out = ema_model.module(x)\n                    else:\n                        out = model(x)\n                \n                val_logits.append(out.cpu())\n                val_targets.append(y.cpu())\n            \n            val_logits = torch.cat(val_logits, dim=0)\n            val_targets = torch.cat(val_targets, dim=0)\n            val_loss = criterion(val_logits, val_targets).item()\n\n        \n        \n        if cfg.use_wandb and wandb is not None and cfg.local_rank == 0:\n            wandb.log({\n                \"val_loss\": val_loss,\n                \"epoch\": epoch,\n                \"best_loss\": best_loss,\n                \"learning_rate\": scheduler.get_last_lr()[0],\n            })\n            \n        scheduler.step() \n        # Save best model\n        if val_loss < best_loss:\n            print(f\"New best: {best_loss:.2f} -> {val_loss:.2f}\")\n            print(\"Saved weights..\")\n            best_loss = val_loss\n            \n            if ema_model is not None:\n                torch.save(ema_model.module.state_dict(), f'best_model_{cfg.seed}.pt')\n            else:\n                torch.save(model.state_dict(), f'best_model_{cfg.seed}.pt')\n            \n            cfg.early_stopping[\"streak\"] = 0\n        else:\n            cfg.early_stopping[\"streak\"] += 1\n            if cfg.early_stopping[\"streak\"] > cfg.early_stopping[\"patience\"]:\n                print(\"Ending training (early_stopping).\")\n                break\n    \n    if cfg.use_wandb and wandb is not None:\n        wandb.finish()\n\ndef main_multi_gpu(cfg):\n    \"\"\"Multi GPU training\"\"\"\n    \n    # Print mode info (only rank 0)\n    if cfg.local_rank == 0:\n        mode_str = \"72x72\" if cfg.use_72x72_mode else \"Full Resolution\"\n        print(f\"Training Mode: {mode_str}\")\n    \n    if cfg.use_wandb and wandb is not None and cfg.local_rank == 0:\n        run_name = f\"{cfg.run_name}_{cfg.seed}\"\n        if cfg.use_72x72_mode:\n            run_name += \"_72x72\"\n        wandb.init(\n            project=cfg.wandb_project,\n            config=vars(cfg),\n            name=run_name\n        )\n\n    # ========== Datasets / Dataloaders ==========\n    if cfg.local_rank == 0:\n        print(\"=\"*25)\n        print(\"Loading data..\")\n    train_ds = CustomDataset(cfg=cfg, mode=\"train\")\n    sampler= DistributedSampler(\n        train_ds, \n        num_replicas=cfg.world_size, \n        rank=cfg.local_rank,\n    )\n    train_dl = torch.utils.data.DataLoader(\n        train_ds, \n        sampler= sampler,\n        batch_size= cfg.batch_size, \n        num_workers= 4,\n    )\n    \n    valid_ds = CustomDataset(cfg=cfg, mode=\"valid\")\n    sampler= DistributedSampler(\n        valid_ds, \n        num_replicas=cfg.world_size, \n        rank=cfg.local_rank,\n    )\n    valid_dl = torch.utils.data.DataLoader(\n        valid_ds, \n        sampler= sampler,\n        batch_size= cfg.batch_size_val, \n        num_workers= 4,\n    )\n\n    # ========== Model / Optim ==========\n    model = Net(backbone=cfg.backbone, use_72x72_mode=cfg.use_72x72_mode, use_smart_256x72_mode=cfg.use_smart_256x72_mode)\n\n    model= model.to(cfg.local_rank)\n    \n    # Disable compilation for 72x72 mode\n    compile_enabled = cfg.compile_model\n    if compile_enabled:\n        model = torch.compile(model, mode='default')\n    if cfg.ema:\n        if cfg.local_rank == 0:\n            print(\"Initializing EMA model..\")\n        ema_model = ModelEMA(\n            model, \n            decay=cfg.ema_decay, \n            device=cfg.local_rank,\n        )\n    else:\n        ema_model = None\n    model= DistributedDataParallel(\n        model, \n        device_ids=[cfg.local_rank],\n        find_unused_parameters=True\n        )\n    \n    criterion = nn.L1Loss()\n    compile_enabled = cfg.compile_model\n    if compile_enabled:\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, fused=True)\n    else:\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    scheduler = CosineAnnealingLR(optimizer, T_max=cfg.epochs)\n\n    if cfg.mixed_precision and cfg.use_grad_scaler:\n        scaler = GradScaler()\n    else:\n        scaler = None\n\n    # ========== Training ==========\n    if cfg.local_rank == 0:\n        print(\"=\"*25)\n        print(\"Give me warp {}, Mr. Sulu.\".format(cfg.world_size))\n        print(\"=\"*25)\n    \n    best_loss= 1_000_000\n    val_loss= 1_000_000\n\n    for epoch in range(0, cfg.epochs+1):\n        if epoch != 0:\n            tstart= time.time()\n            train_dl.sampler.set_epoch(epoch)\n    \n            # Train loop\n            model.train()\n            total_loss = []\n            for i, (x, y) in enumerate(train_dl):\n                x = x.to(cfg.local_rank)\n                y = y.to(cfg.local_rank)\n        \n                if cfg.mixed_precision:\n                    autocast_context = autocast(device_type='cuda', dtype=cfg.precision_dtype)\n                else:\n                    autocast_context = nullcontext()\n                \n                with autocast_context:\n                    logits = model(x)\n\n                def smoothness_loss(pred_vel, alpha=0.1):\n                        grad_x = torch.abs(pred_vel[:, :, :, 1:] - pred_vel[:, :, :, :-1])\n                        grad_y = torch.abs(pred_vel[:, :, 1:, :] - pred_vel[:, :, :-1, :])\n                        return alpha * (grad_x.mean() + grad_y.mean())\n\n                if cfg.smoothness_loss:\n                    # With:\n                    mae_loss = criterion(logits, y)\n                    smooth_loss = smoothness_loss(logits, alpha=0.1)\n                    loss = mae_loss + smooth_loss\n                else:\n                    loss = criterion(logits, y)\n\n        \n                if scaler is not None:\n                    scaler.scale(loss).backward()\n                    scaler.unscale_(optimizer)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n                    scaler.step(optimizer)\n                    scaler.update()\n                    optimizer.zero_grad()\n                else:\n                    loss.backward()\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n                    optimizer.step()\n                    optimizer.zero_grad()\n    \n                total_loss.append(loss.item())\n                \n                if ema_model is not None:\n                    ema_model.update(model)\n                    \n                if cfg.local_rank == 0 and (len(total_loss) >= cfg.logging_steps or i == 0):\n                    train_loss = np.mean(total_loss)\n                    total_loss = []\n                    print(\"Epoch {}:     Train MAE: {:.2f}     Val MAE: {:.2f}     Time: {}     Step: {}/{}\".format(\n                        epoch, \n                        train_loss,\n                        val_loss,\n                        format_time(time.time() - tstart),\n                        i+1, \n                        len(train_dl)+1, \n                    ))\n\n                if cfg.use_wandb and wandb is not None and cfg.local_rank == 0 and i % 50 == 0:\n                    wandb.log({\n                        \"train_loss\": loss.item(),\n                        \"epoch\": epoch,\n                        \"step\": epoch * len(train_dl) + i\n                    })\n    \n        # ========== Valid ==========\n        model.eval()\n        val_logits = []\n        val_targets = []\n        \n        if cfg.mixed_precision:\n            autocast_context = autocast(device_type='cuda', dtype=cfg.precision_dtype)\n        else:\n            autocast_context = nullcontext()\n            \n        with torch.no_grad():\n            for x, y in tqdm(valid_dl, disable=cfg.local_rank != 0):\n                x = x.to(cfg.local_rank)\n                y = y.to(cfg.local_rank)\n    \n                with autocast_context:\n                    if ema_model is not None:\n                        out = ema_model.module(x)\n                    else:\n                        out = model(x)\n\n                val_logits.append(out.cpu())\n                val_targets.append(y.cpu())\n\n            val_logits= torch.cat(val_logits, dim=0)\n            val_targets= torch.cat(val_targets, dim=0)\n                \n            loss = criterion(val_logits, val_targets).item()\n\n        # Gather loss\n        v = torch.tensor([loss], device=cfg.local_rank)\n        torch.distributed.all_reduce(v, op=dist.ReduceOp.SUM)\n        val_loss = (v[0] / cfg.world_size).item()\n        \n        if cfg.use_wandb and wandb is not None and cfg.local_rank == 0:\n            wandb.log({\n                \"val_loss\": val_loss,\n                \"epoch\": epoch,\n                \"best_loss\": best_loss,\n                \"learning_rate\": scheduler.get_last_lr()[0],\n            })\n\n        scheduler.step()\n    \n        # ========== Weights / Early stopping ==========\n        stop_train = torch.tensor([0], device=cfg.local_rank)\n        if cfg.local_rank == 0:\n            es= cfg.early_stopping\n            if val_loss < best_loss:\n                print(\"New best: {:.2f} -> {:.2f}\".format(best_loss, val_loss))\n                print(\"Saved weights..\")\n                best_loss = val_loss\n                if ema_model is not None:\n                    torch.save(ema_model.module.state_dict(), f'best_model_{cfg.seed}.pt')\n                else:\n                    torch.save(model.state_dict(), f'best_model_{cfg.seed}.pt')\n        \n                es[\"streak\"] = 0\n            else:\n                es= cfg.early_stopping\n                es[\"streak\"] += 1\n                if es[\"streak\"] > es[\"patience\"]:\n                    print(\"Ending training (early_stopping).\")\n                    stop_train = torch.tensor([1], device=cfg.local_rank)\n        \n        # Exits training on all ranks\n        dist.broadcast(stop_train, src=0)\n        if stop_train.item() == 1:\n            return\n\n    if cfg.use_wandb and wandb is not None and cfg.local_rank == 0:\n        wandb.finish()\n\n    return\n\nif __name__ == \"__main__\":\n    \n    set_seed(cfg.seed)\n    \n    # Auto-detect GPU configuration\n    setup_gpu_mode(cfg)\n    \n    if cfg.distributed:\n        # Multi-GPU setup\n        setup(cfg.local_rank, cfg.world_size)\n        \n        _, total = torch.cuda.mem_get_info(device=cfg.local_rank)\n        time.sleep(cfg.local_rank)\n        print(f\"Rank: {cfg.local_rank}, World size: {cfg.world_size}, GPU memory: {total / 1024**3:.2f}GB\", flush=True)\n        time.sleep(cfg.world_size - cfg.local_rank)\n        \n        set_seed(cfg.seed + cfg.local_rank)  # Different seed per GPU\n        \n        main_multi_gpu(cfg)\n        cleanup()\n    else:\n        # Single GPU setup\n        main_single_gpu(cfg)","metadata":{"_uuid":"f94e41ce-af9c-40ca-9f7a-bd7c99450610","_cell_guid":"e81e2e39-fec7-4b10-886e-69358810fef3","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-30T16:14:36.400938Z","iopub.execute_input":"2025-06-30T16:14:36.401208Z","iopub.status.idle":"2025-06-30T16:14:36.413227Z","shell.execute_reply.started":"2025-06-30T16:14:36.401192Z","shell.execute_reply":"2025-06-30T16:14:36.412614Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Overwriting _train.py\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"","metadata":{"_uuid":"5f601c88-099b-421e-99da-ce31af0d27ad","_cell_guid":"6b832d10-cf6a-492c-8529-06ad7a50e97d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if RUN_TRAIN:\n    print(\"Starting training..\")\n    !OMP_NUM_THREADS=1 torchrun --nproc_per_node=2 _train.py\n    # single‐GPU:\n    #!python _train.py","metadata":{"_uuid":"006ad3f5-a183-4bb7-8d71-05e5161a7b6b","_cell_guid":"70c49561-7624-47c2-b2c7-d733d9ab1d47","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-30T16:14:38.352328Z","iopub.execute_input":"2025-06-30T16:14:38.352944Z","execution_failed":"2025-06-30T16:22:52.894Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Starting training..\n<frozen importlib._bootstrap_external>:1241: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n<frozen importlib._bootstrap_external>:1241: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n2025-06-30 16:14:49.452285: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-06-30 16:14:49.452418: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751300089.479634    6203 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751300089.479634    6202 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751300089.487307    6202 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nE0000 00:00:1751300089.487453    6203 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshah1st-work-ua\u001b[0m (\u001b[33mshah1st-work-ua-igor-sikorsky-kyiv-polytechnic-institute\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n[Distributed] Rank: 0, World size: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshah1st-work-ua\u001b[0m (\u001b[33mshah1st-work-ua-igor-sikorsky-kyiv-polytechnic-institute\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n[Distributed] Rank: 1, World size: 2\nRank: 0, World size: 2, GPU memory: 14.74GB\nRank: 1, World size: 2, GPU memory: 14.74GB\nTraining Mode: Full Resolution\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250630_161455-bb4v3hzo\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mconvnext_0.3_[-2]layer_123\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/shah1st-work-ua-igor-sikorsky-kyiv-polytechnic-institute/seismic-inversion\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/shah1st-work-ua-igor-sikorsky-kyiv-polytechnic-institute/seismic-inversion/runs/bb4v3hzo\u001b[0m\n=========================\nLoading data..\n100%|█████████████████████████████████████████| 276/276 [00:03<00:00, 77.58it/s]\n100%|███████████████████████████████████████████| 10/10 [00:00<00:00, 88.75it/s]\n[Full Res Mode] Replacing all activations with GELU...\n[Full Res Mode] Replacing all norms with InstanceNorm...\n[Full Res Mode] Replacing forward functions...\n[Full Res Mode] Replacing all activations with GELU...\n[Full Res Mode] Replacing all norms with InstanceNorm...\n[Full Res Mode] Replacing forward functions...\nInitializing EMA model..\n=========================\nGive me warp 2, Mr. Sulu.\n=========================\n100%|███████████████████████████████████████████| 79/79 [02:08<00:00,  1.63s/it]\nNew best: 1000000.00 -> 836.76\nSaved weights..\nEpoch 1:     Train MAE: 804.24     Val MAE: 836.76     Time: 0:00:53     Step: 1/2158\nEpoch 1:     Train MAE: 448.03     Val MAE: 836.76     Time: 0:03:26     Step: 101/2158\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"### Pretrained Models\n\nNext, we load in 2x pretrained models. These models were trained with a batch size of 16 for 50 epochs.","metadata":{"_uuid":"ed4fb039-cda5-4155-949b-c2803c8e488e","_cell_guid":"8e099528-928d-41f3-9318-bf0efca779c6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import glob\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom _cfg import cfg\nfrom _model import Net, EnsembleModel\n\nif RUN_VALID or RUN_TEST:\n\n    # Load pretrained models\n    models = []\n    for f in sorted(glob.glob(\"/kaggle/working/*.pt\")):\n        print(\"Loading: \", f)\n        m = Net(\n            backbone=cfg.backbone,\n            pretrained=False,\n            use_72x72_mode=cfg.use_72x72_mode, \n            use_smart_256x72_mode=cfg.use_smart_256x72_mode\n        )\n\n        state_dict= torch.load(f, map_location=cfg.device, weights_only=True)\n        state_dict= {k.removeprefix(\"_orig_mod.\"):v for k,v in state_dict.items()} # Remove torch.compile() prefix\n\n        m.load_state_dict(state_dict)\n        if cfg.compile_model:\n            m = torch.compile(m, mode='reduce-overhead')\n        models.append(m)\n    \n    # Combine\n    model = EnsembleModel(models)\n    model = model.to(cfg.device)\n    model = model.eval()\n    print(\"n_models: {:_}\".format(len(models)))\n\n    for name, param in model.named_parameters():\n        if torch.isnan(param).any():\n            print(f\"NaN detected in {name}\")\n            break","metadata":{"_uuid":"231c87c7-d422-4a42-b45d-7258789b475e","_cell_guid":"95efca84-0b9d-4535-8b4d-0fba464fd288","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-30T16:10:21.955207Z","iopub.execute_input":"2025-06-30T16:10:21.955497Z","iopub.status.idle":"2025-06-30T16:10:22.932715Z","shell.execute_reply.started":"2025-06-30T16:10:21.955446Z","shell.execute_reply":"2025-06-30T16:10:22.931528Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Loading:  /kaggle/working/best_model_123.pt\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/2800332631.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/working/*.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         m = Net(\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mbackbone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, backbone, pretrained, use_72x72_mode, use_smart_256x72_mode)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;31m# Decoder - DIFFERENT for each mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0muse_smart_256x72_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m             \u001b[0;31m# STANDARD DECODER: Works perfectly with engineered 32×32 stem output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             self.decoder= UnetDecoder2d(\n","\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'decoder_channels' where it is not associated with a value"],"ename":"UnboundLocalError","evalue":"cannot access local variable 'decoder_channels' where it is not associated with a value","output_type":"error"}],"execution_count":31},{"cell_type":"markdown","source":"# Valid\n\nNext, we score the ensemble on the validation set.","metadata":{"_uuid":"65a7f1e4-e412-4874-9043-e780ae14c6f3","_cell_guid":"70a75333-6b55-4535-a34f-f27c016e3820","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from tqdm import tqdm\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.amp import autocast\n\nfrom _dataset import CustomDataset\nimport copy\n\n\nfrom _cfg import cfg\n\n\nif RUN_VALID:\n    cfg_full_val = copy.deepcopy(cfg)\n    cfg_full_val.subsample_fraction = None  # Use full validation set\n    \n    valid_ds = CustomDataset(cfg=cfg_full_val, mode=\"valid\")\n\n    \n    sampler = torch.utils.data.SequentialSampler(valid_ds)\n    valid_dl = torch.utils.data.DataLoader(\n        valid_ds, \n        sampler= sampler,\n        batch_size= cfg.batch_size_val, \n        num_workers= 4,\n    )\n\n    # Valid loop\n    criterion = nn.L1Loss()\n    val_logits = []\n    val_targets = []\n    \n    with torch.no_grad():\n        for x, y in tqdm(valid_dl):\n            x = x.to(cfg.device)\n            y = y.to(cfg.device)\n\n            if torch.isnan(x).any() or torch.isnan(y).any():\n                print(\"NaN in input data\")\n                continue\n    \n            # To this:\n            if cfg.mixed_precision:\n                with autocast(device_type='cuda', dtype=cfg.precision_dtype):\n                    out = model(x)\n            else:\n                out = model(x)\n    \n            val_logits.append(out.cpu())\n            val_targets.append(y.cpu())\n    \n        val_logits= torch.cat(val_logits, dim=0)\n        val_targets= torch.cat(val_targets, dim=0)\n    \n        total_loss= criterion(val_logits, val_targets).item()\n    \n    # Dataset Scores\n    ds_idxs= np.array([valid_ds.records])\n    ds_idxs= np.repeat(ds_idxs, repeats=500)\n    \n    print(\"=\"*25)\n    with torch.no_grad():    \n        for idx in sorted(np.unique(ds_idxs)):\n    \n            # Mask\n            mask = ds_idxs == idx\n            logits_ds = val_logits[mask]\n            targets_ds = val_targets[mask]\n    \n            # Score predictions\n            loss = criterion(val_logits[mask], val_targets[mask]).item()\n            print(\"{:15} {:.2f}\".format(idx, loss))\n    print(\"=\"*25)\n    print(\"Val MAE: {:.2f}\".format(total_loss))\n    print(\"=\"*25)","metadata":{"_uuid":"bbd8dadb-7a5c-42f0-a048-b2d9b5ad2cf8","_cell_guid":"316d45cd-0daf-46f3-8dae-5ff845a47829","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-30T16:10:22.933284Z","iopub.status.idle":"2025-06-30T16:10:22.933591Z","shell.execute_reply.started":"2025-06-30T16:10:22.933413Z","shell.execute_reply":"2025-06-30T16:10:22.933432Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test\n\nFinally, we make predictions on the test data.","metadata":{"_uuid":"898080a5-1e82-4797-9889-496902119f88","_cell_guid":"b49da319-db28-4bdc-8388-1cde72b605a0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import torch\n\nclass TestDataset(torch.utils.data.Dataset):\n    def __init__(self, test_files):\n        self.test_files = test_files\n\n    def __len__(self):\n        return len(self.test_files)\n\n    def __getitem__(self, i):\n        test_file = self.test_files[i]\n        test_stem = test_file.split(\"/\")[-1].split(\".\")[0]\n        x = np.load(test_file)\n        \n        # Add the same dtype conversion as CustomDataset\n        if not cfg.mixed_precision:\n            x = x.astype(np.float32)\n        \n        return x, test_stem","metadata":{"_uuid":"259173c6-09b8-4acb-9563-64041f10505d","_cell_guid":"b9e4ebf3-d1f5-4131-8ee5-7bc558c2fdcd","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-30T16:10:22.935236Z","iopub.status.idle":"2025-06-30T16:10:22.935603Z","shell.execute_reply.started":"2025-06-30T16:10:22.935403Z","shell.execute_reply":"2025-06-30T16:10:22.935419Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import csv\nimport time\nimport glob\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\n\nfrom _utils import format_time\n\n\nif RUN_TEST:\n\n    def _preprocess(x):\n        x = F.interpolate(x, size=(70, 70), mode='area')\n        x = F.pad(x, (1,1,1,1), mode='replicate')\n        return x\n    \n\n    ss= pd.read_csv(\"/kaggle/input/waveform-inversion/sample_submission.csv\")    \n    row_count = 0\n    t0 = time.time()\n    \n    test_files = sorted(glob.glob(\"/kaggle/input/open-wfi-test/test/*.npy\"))\n    x_cols = [f\"x_{i}\" for i in range(1, 70, 2)]\n    fieldnames = [\"oid_ypos\"] + x_cols\n    \n    test_ds = TestDataset(test_files)\n    test_dl = torch.utils.data.DataLoader(\n        test_ds, \n        sampler=torch.utils.data.SequentialSampler(test_ds),\n        batch_size=cfg.batch_size_val, \n        num_workers=4,\n    )\n    \n    with open(\"submission.csv\", \"wt\", newline=\"\") as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n\n        with torch.inference_mode():\n            with torch.autocast(cfg.device.type):\n                for inputs, oids_test in tqdm(test_dl, total=len(test_dl)):\n                    if not cfg.mixed_precision:\n                        inputs = inputs.to(cfg.device, dtype=torch.float32)  # Explicit dtype conversion\n                    \n                    if cfg.use_72x72_mode:\n                        inputs = _preprocess(inputs)  # Added 72x72 support\n                    outputs = model(inputs)\n            \n                            \n                    y_preds = outputs[:, 0].cpu().numpy()\n                    \n                    for y_pred, oid_test in zip(y_preds, oids_test):\n                        for y_pos in range(70):\n                            row = dict(zip(x_cols, [y_pred[y_pos, x_pos] for x_pos in range(1, 70, 2)]))\n                            row[\"oid_ypos\"] = f\"{oid_test}_y_{y_pos}\"\n            \n                            writer.writerow(row)\n                            row_count += 1\n\n                            # Clear buffer\n                            if row_count % 100_000 == 0:\n                                csvfile.flush()\n    \n    t1 = format_time(time.time() - t0)\n    print(f\"Inference Time: {t1}\")","metadata":{"_uuid":"3376fa59-1b7c-458a-8904-0fb472ae8ffd","_cell_guid":"e4a6c6bd-5956-481f-a428-bfd2124dde07","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-30T15:44:23.352339Z","iopub.status.idle":"2025-06-30T15:44:23.352645Z","shell.execute_reply.started":"2025-06-30T15:44:23.352521Z","shell.execute_reply":"2025-06-30T15:44:23.352534Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can also view a few samples to make sure things look reasonable.","metadata":{"_uuid":"d5db1ed9-6b39-493b-987e-4e81c533e059","_cell_guid":"e16bde3c-7023-422f-8337-e378ec697201","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"","metadata":{"_uuid":"81505e3e-a2c4-4756-a08b-7da5d9f18f02","_cell_guid":"fb6fd3fd-c6cf-4df0-adf7-45f19f660dea","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}